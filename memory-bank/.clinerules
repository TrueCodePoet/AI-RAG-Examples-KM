# .clinerules for AI-RAG-Examples-KM

## Project Patterns

### Code Organization
- The project follows a modular architecture with clear separation of concerns
- Core components are organized in the root directory
- Specialized implementations are in subdirectories (AzureCosmosDb, AzureCosmosDbTabular)
- Configuration is centralized in appsettings.json and loaded via KernelSetup.cs

### Dependency Injection
- Services are registered in the DependencyInjection.cs file
- Both interfaces and concrete implementations are registered when needed for direct resolution
- Factory patterns are used for components that require runtime configuration
- When a component needs a specific implementation (not just an interface), register the concrete type and use GetService<ConcreteType>() first, then fall back to casting from the interface
- Prefer using interfaces (e.g., IMemoryDb) over concrete implementations (e.g., AzureCosmosDbTabularMemory) in method signatures and class fields to improve maintainability and allow for easier swapping of implementations

### Naming Conventions
- Class names are descriptive and follow PascalCase (e.g., BlobStorageProcessor)
- Interface names start with "I" (e.g., IContentDecoder)
- Private fields use underscore prefix (e.g., _memory)
- Constants use UPPER_CASE or PascalCase depending on scope
- Configuration classes use descriptive names with "Config" or "Settings" suffix

### Error Handling
- Exceptions are caught and logged with meaningful messages
- Processing continues where possible after non-critical errors
- Azure service errors are handled specifically with appropriate error messages
- Console output is used for status updates and error reporting

### Documentation
- XML documentation is used for public APIs
- Comments explain complex logic or non-obvious implementations
- Console output provides progress information during execution
- README files provide overview information for directories

## Implementation Details

*(Note: The source repository `X:\AI\Kernel-Memory-Clone\kernel-memory\extensions` contains both a standard `AzureCosmosDb` vector store extension and the specialized `AzureCosmosDbTabular` extension used in this project.)*

### Tabular Data Processing
- Excel files are processed using the `TabularExcelDecoder`.
- CSV files are processed using the `TabularCsvDecoder`, which is modeled after `TabularExcelDecoder`.
- **Crucial:** For proper integration and data extraction, both decoders must construct the `text` field in the **exact same sentence format**.
- Text representation follows the format: `"Record from worksheet {worksheetName}, row {rowNumber}: schema_id is {schemaId}. import_batch_id is {importBatchId}. {Column1} is {Value1}. {Column2} is {Value2}."`
- This specific format is parsed by `AzureCosmosDbTabularMemoryRecord.ParseSentenceFormat` to populate the `Data` and `Source` fields. **Any deviation will result in empty `Data` fields.**
- Data types are preserved when `PreserveDataTypes = true` in the configuration.
- Headers are normalized to remove special characters and ensure valid field names.
- Each row becomes a separate memory record with preserved structure in the `Data` field (if parsing succeeds).
- PivotTable structures in Excel files are handled gracefully with the `SkipPivotTables` configuration option (default: true).
- Source file names for schema generation prioritize the provided file path parameter over the workbook's internal Title property.
- The `ParseSentenceFormat` method handles concatenated text by detecting multiple record patterns and truncating to process only the first record.

### Lessons Learned (Do Not Remove - Added 2025-04-16)
- **Text Format Consistency is Critical:** The "text" field format generated by *all* tabular decoders (Excel, CSV, etc.) MUST exactly match the format expected by `AzureCosmosDbTabularMemoryRecord.ParseSentenceFormat`. This includes the prefix `Record from worksheet ...`, the key-value structure `{key} is {value}.`, and the inclusion of `schema_id` and `import_batch_id`. Failure to match results in empty `Data` fields in Cosmos DB.
- **Base64 Id Encoding:** All record Ids *must* be encoded using `AzureCosmosDbTabularMemoryRecord.EncodeId` before being stored. If a raw ID is stored, `DecodeId` will throw a `System.FormatException` during retrieval because the input is not valid Base64.
- **CSV Row Skipping:** The `TabularCsvDecoder` might skip rows if the `HeaderRowIndex` configuration is incorrect or if the CSV contains blank lines before the data. Added logging (`totalLines`, `totalRowsAdded`) helps diagnose this.
- **Vector Size Consistency:** The `vectorSize` parameter for index creation must be set to the correct embedding dimension (e.g., 1536) for *all* tabular data types (Excel, CSV). If `vectorSize` is 0 or inconsistent, Cosmos DB vector search operations may fail or return incorrect results.
- **Index/Container Creation:** The target Cosmos DB container must exist *before* attempting to store schema or row data. Ensure creation logic runs first.
- **Pipeline File Filtering:** When using multiple pipelines (e.g., standard and tabular), ensure the tabular pipeline is configured to explicitly process relevant file types (e.g., `*.xlsx`, `*.csv`) using `WithFiles()` or similar configuration.
- **DI Consistency:** Both Excel and CSV decoders should receive the database instance (`IMemoryDb`) via the same Dependency Injection pattern. Differences in behavior usually stem from pipeline configuration, not DI setup.
- **Schema and Row Index Consistency:** Both schema records and data row records *must* be stored in the same Cosmos DB container/index for correct querying and association via `schema_id`.

### Metadata Extraction
- The `TabularExcelDecoder` and `TabularCsvDecoder` generate text in the specific format described under "Tabular Data Processing".
- `AzureCosmosDbTabularMemoryRecord` parses this text to extract metadata (worksheet, row, schema_id, import_batch_id) and row data.
- When modifying decoders or the parser, ensure the text format and parsing logic remain synchronized.

### Data Deserialization
- `AzureCosmosDbTabularMemoryRecord.ParseSentenceFormat` extracts data from the "text" field.
- It converts string values to appropriate types (boolean, integer, double, or string).
- Populates the `Data` dictionary with extracted key-value pairs (using normalized keys).
- Modifications to text format require corresponding updates to the parsing logic.

### Filter Generation
- Natural language queries are analyzed to extract structured filters.
- Filter generation is a multi-step process handled by `KernelMemoryQueryProcessor`.
- Key filter generation rules:
  - Field names for structured data must be prefixed with `data.` (e.g., `data.column_name`).
  - Field names must be normalized to snake_case format (e.g., "Server Purpose" becomes `data.server_purpose`).
  - Output must be a valid JSON object.
- `AzureCosmosDbTabularMemory` normalizes field names before building the SQL query.

### Query Processing
- Queries can use direct search or AI-assisted approaches.
- Results include source attribution.
- Response formatting is handled by `KernelMemoryQueryProcessor`.

## Configuration Requirements

### Azure OpenAI
- Requires separate configurations for text and embedding models.
- `Auth` type must be set to `APIKey`.
- Deployment names must match valid deployments.
- Rate limiting: System implements retry with backoff. Consider `MaxRetries`, `MaxTokensPerMinute`, `MaxRequestsPerMinute`.

### Azure Cosmos DB
- Requires endpoint and API key.
- Default database name is "memory".
- **Indexing:** Container policy *must* include `/data/*` for filtering. Exclude `/embedding/*`. Vector indexing policy must be configured correctly (e.g., `vectorSize: 1536`).
- **Schema Storage:** Schemas stored in the same container as data, identified by `document_type: "schema"`.

### Azure Blob Storage
- Configured with container URL and optional SAS token.
- Local download path must exist.

## Testing Approach

### Sample Queries
- "Give me a list of all server names with a Server Purpose of 'Corelight Network monitoring sensor'"
- "What servers are running Windows Server 2019 in the East US location?"
- "What is the server purpose for VAXVNAGG01?"

### Test Data
- Excel and CSV files with server information are used.
- Files should include columns for server name, purpose, environment, etc.
- Both local files and blob storage can be used as data sources.

### Dual Pipeline Processing
- Application maintains two pipelines:
  1. **Tabular Pipeline**: Uses `AzureCosmosDbTabularMemory`, omits `TextPartitioningHandler`. Preserves row structure.
  2. **Standard Pipeline**: Uses `AzureCosmosDbMemory`, includes `TextPartitioningHandler`. For general semantic search.
- Each uses a different Cosmos DB container (index name).
- Documents can be processed through both. Queries directed accordingly.

## Common Workflows

### Adding a New Document Source
1. Configure in appsettings.json.
2. Create `BlobStorageProcessor` instances for both pipelines.
3. Call `ProcessBlobsFromStorageAsync()` or `ProcessFilesFromLocalDirectoryAsync()`.

### Implementing a Custom Decoder
1. Create class implementing `IContentDecoder`.
2. Implement `SupportsMimeType()` and `DecodeAsync()`.
3. Register in the memory pipeline builder.
4. **Crucial:** Ensure `DecodeAsync` produces the exact "text" format required by `AzureCosmosDbTabularMemoryRecord.ParseSentenceFormat`.

### Adding a New Query Type
1. Extend filter generation prompt.
2. Update `KernelMemoryQueryProcessor`.
3. Test with sample queries.

## Troubleshooting Guide

### Common Issues

#### "Failed to load Excel file"
- Check for PivotTables (use `SkipPivotTables: true`).
- Verify file is not corrupted/password-protected.
- Try resaving in Excel.

#### "Could not access the memory DB instance"
- Verify `IMemoryDb` instance is initialized and resolved correctly via DI.
- Check configuration loading.
- Ensure correct `IMemoryDb` implementation is used for the pipeline.

#### "Filter generation error"
- Check query format and filter prompt.
- Look for ambiguous field names.

#### "Type compatibility issues"
- Be mindful of internal/public type accessibility when extending framework components.

#### "Collection type pattern matching issues"
- C# 8 pattern matching can fail with nullable generics. Use `GetType()`, `IsAssignableFrom`, and `GetEnumerator()` for robust collection handling.

#### "Empty source dictionary" / "Empty data dictionary" / "Data parsing artifacts"
- **Primary Cause:** Mismatch between the "text" field format generated by the decoder and the format expected by `AzureCosmosDbTabularMemoryRecord.ParseSentenceFormat`.
- **Solution:** Ensure the decoder (Excel or CSV) produces the exact sentence format: `"Record from worksheet {ws}, row {row}: schema_id is {sid}. import_batch_id is {bid}. {Key1} is {Value1}. {Key2} is {Value2}."`. Verify prefix, spacing, punctuation, and inclusion of all fields. Use logging in both the decoder and parser to compare generated text vs. expected format.

#### "Azure service errors"
- Verify API keys/endpoints. Check Azure status. Check rate limits.

#### "Rate limiting warnings in logs"
- Normal for large datasets. System retries with backoff. Consider rate limit config or batching.

#### "Could not access the memory DB instance" / "Failed to extract AzureCosmosDbTabularMemory"
- **Cause**: Components needing specific `IMemoryDb` implementation but only having `IKernelMemory`. Reflection is unreliable.
- **Solution**: Use DI container (`KernelMemoryBuilder`) to resolve `IMemoryDb` explicitly during initialization and pass the instance directly.

#### Reflection Usage (Obsolete/Warning)
- Reflection (`MemoryHelper`, `GetMemoryDbFromKernelMemory`, `SetMemoryOnTabularExcelDecoders`) is **unreliable and brittle**, especially with `MemoryServerless`. **Avoid if possible.** Prefer DI-based resolution. If reflection is absolutely necessary, acknowledge its fragility.

#### Fault Tolerance for Memory Operations (Added 2025-04-14)
- Components updated to handle unavailable memory DB gracefully (e.g., skip dataset identification). Uses `MemoryHelper` for centralized (but still fragile) reflection. DI is the preferred long-term fix.

#### System.FormatException: 'Input is not a valid Base-64 string' (Added 2025-04-16)
- **Cause:** Calling `AzureCosmosDbTabularMemoryRecord.DecodeId` on a record `Id` that was not previously encoded using `EncodeId`.
- **Solution:** Ensure *all* code paths that create or update `AzureCosmosDbTabularMemoryRecord` instances correctly use `EncodeId(record.Id)` when setting the `Id` property before saving to Cosmos DB. Check `FromMemoryRecord` and any direct record creation logic.
