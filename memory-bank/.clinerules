# .clinerules for AI-RAG-Examples-KM

## Project Patterns

### Code Organization
- The project follows a modular architecture with clear separation of concerns
- Core components are organized in the root directory
- Specialized implementations are in subdirectories (AzureCosmosDb, AzureCosmosDbTabular)
- Configuration is centralized in appsettings.json and loaded via KernelSetup.cs

### Dependency Injection
- Services are registered in the DependencyInjection.cs file
- Both interfaces and concrete implementations are registered when needed for direct resolution
- Factory patterns are used for components that require runtime configuration
- When a component needs a specific implementation (not just an interface), register the concrete type and use GetService<ConcreteType>() first, then fall back to casting from the interface
- Prefer using interfaces (e.g., IMemoryDb) over concrete implementations (e.g., AzureCosmosDbTabularMemory) in method signatures and class fields to improve maintainability and allow for easier swapping of implementations

### Naming Conventions
- Class names are descriptive and follow PascalCase (e.g., BlobStorageProcessor)
- Interface names start with "I" (e.g., IContentDecoder)
- Private fields use underscore prefix (e.g., _memory)
- Constants use UPPER_CASE or PascalCase depending on scope
- Configuration classes use descriptive names with "Config" or "Settings" suffix

### Error Handling
- Exceptions are caught and logged with meaningful messages
- Processing continues where possible after non-critical errors
- Azure service errors are handled specifically with appropriate error messages
- Console output is used for status updates and error reporting

### Documentation
- XML documentation is used for public APIs
- Comments explain complex logic or non-obvious implementations
- Console output provides progress information during execution
- README files provide overview information for directories

## Implementation Details

*(Note: The source repository `X:\AI\Kernel-Memory-Clone\kernel-memory\extensions` contains both a standard `AzureCosmosDb` vector store extension and the specialized `AzureCosmosDbTabular` extension used in this project.)*

### Tabular Data Processing
- Excel files are processed using the TabularExcelDecoder
- Data types are preserved when PreserveDataTypes = true
- Headers are normalized to remove special characters
- Each row becomes a separate memory record with preserved structure in the `Data` field.
- Text representation follows the format "Record from worksheet X, row Y: schema_id is abc123. import_batch_id is xyz789. Column1 is Value1. Column2 is Value2." This specific format is parsed by `AzureCosmosDbTabularMemoryRecord` to populate the `Data` and `Source` fields; changes require synchronization between the decoder and the record class.
- The `ParseSentenceFormat` method in `AzureCosmosDbTabularMemoryRecord` uses a combination of regex for the prefix extraction and manual string splitting/parsing for the data section to ensure reliable data extraction.
- Schema information is extracted during import and stored in the same container as the data, with special metadata tags to identify schema documents.
- PivotTable structures in Excel files are now handled gracefully with the SkipPivotTables configuration option (default: true).
- Source file names for schema generation prioritize the provided file path parameter over the workbook's internal Title property. This ensures accurate source file attribution in the schema even when the Excel file's metadata is generic or missing.
- The `ParseSentenceFormat` method now handles concatenated text by detecting multiple record patterns and truncating to process only the first record, preventing data bleeding between records.

### Metadata Extraction
- The TabularExcelDecoder generates text in a specific format that includes metadata about the record:
  - "Record from worksheet SheetName, row 123: schema_id is abc123. import_batch_id is xyz789. Column1 is Value1. Column2 is Value2."
- AzureCosmosDbTabularMemoryRecord parses this text to extract both metadata and row data:
  1. Uses regex to extract the worksheet name and row number from the prefix
  2. Isolates the data section (everything after the colon)
  3. Splits the data section by periods to identify individual key-value pairs
  4. Processes each pair to extract the key and value
  5. Special handling for schema_id and import_batch_id to store them in the source dictionary
  6. Skips keys that look like record prefixes to avoid parsing artifacts
- This approach ensures all metadata and row data are properly extracted from the text field
- When modifying either component, ensure the text format and parsing logic remain synchronized

### Data Deserialization
- The TabularExcelDecoder encodes row data in the text field using the "sentence format"
- AzureCosmosDbTabularMemoryRecord extracts this data through the `ParseSentenceFormat` method:
  1. Parses the text field to extract both metadata and row data
  2. Uses the same parsing logic described in the "Metadata Extraction" section
  3. Converts string values to appropriate types (boolean, integer, double, or string)
  4. Populates the Data dictionary with the extracted key-value pairs
- This approach ensures the memory row holds the actual data with correct fields and values
- When modifying the text format in TabularExcelDecoder, ensure the parsing logic in AzureCosmosDbTabularMemoryRecord is updated accordingly

### Filter Generation
- Natural language queries are analyzed to extract structured filters
- Filter generation is a multi-step process handled by KernelMemoryQueryProcessor:
  1. **Dataset Identification**: Uses TabularFilterHelper to list available datasets and an LLM call to determine which is relevant to the query
  2. **Filter Generation**: Uses a specialized prompt to generate a JSON representation of filters
  3. **Schema-based Validation**: Uses TabularFilterHelper to validate generated filters against the dataset's schema
  4. **Query Execution**: Applies validated filters to Cosmos DB query
- Key filter generation rules enforced by the prompt:
  - Field names for structured data must be prefixed with "data." (e.g., `data.column_name`)
  - Field names must be normalized to snake_case format (e.g., "Server Purpose" becomes `data.server_purpose`)
  - Output must be a valid JSON object with string values (no nested structures)
  - The JSON format is critical as it's directly deserialized to create the filter object
- When querying, the `AzureCosmosDbTabularMemory` normalizes these field names (e.g., converting camelCase `data.serverPurpose` to snake_case `data.server_purpose`) before building the SQL query, assuming snake_case keys in the stored `Data` object
- The filter generation prompt includes examples for common query patterns
- Generated filters are applied to the Cosmos DB query

### Query Processing
- Queries can use direct search or AI-assisted approaches
- Results include source attribution with document details
- Response formatting is handled by the KernelMemoryQueryProcessor
- The system can handle both general questions and specific data queries

## Configuration Requirements

### Azure OpenAI
- Requires separate configurations for text and embedding models
- Auth type must be set to APIKey after binding from configuration
- Deployment names must match valid deployments in the Azure OpenAI service
- Rate limiting considerations:
  - Processing large datasets can trigger Azure OpenAI rate limits
  - The system implements retry with backoff based on Retry-After headers
  - Consider adding MaxRetries, MaxTokensPerMinute, and MaxRequestsPerMinute settings

### Azure Cosmos DB
- Requires endpoint and API key.
- Default database name is "memory" if not specified via `AzureCosmosDbTabularConfig`.
- **Indexing:** For the `AzureCosmosDbTabular` extension, the container indexing policy *must* include the `/data/*` path to enable filtering on structured fields. The `/embedding/*` path should generally be excluded from standard indexing.
- **Schema Storage:** Schema information is stored in the same container as the data, using a "document_type" metadata tag to identify schema documents. This approach simplifies the architecture and reduces the number of containers needed.

### Azure Blob Storage
- Configured with container URL and optional SAS token
- Local download path must exist or be created before processing

## Testing Approach

### Sample Queries
- "Give me a list of all server names with a Server Purpose of 'Corelight Network monitoring sensor'"
- "What servers are running Windows Server 2019 in the East US location?"
- "What is the server purpose for VAXVNAGG01?"

### Test Data
- Excel files with server information are used for testing
- Files should include columns for server name, purpose, environment, etc.
- Both local files and blob storage can be used as data sources

### Dual Pipeline Processing 
- The application is now capable of maintaining two separate Kernel Memory pipelines:
  1. **Tabular Pipeline**: Uses `AzureCosmosDbTabularMemory` and omits the `TextPartitioningHandler` to preserve row-based chunks intact. Optimal for structured data like Excel files where preserving the exact structure is important.
  2. **Standard Pipeline**: Uses the default `AzureCosmosDbMemory` and includes the `TextPartitioningHandler` for optimal embedding-based search across all content. Better for general text documents and semantic search.
- Each pipeline uses a different Cosmos DB container (via different index names) to store its records
- Documents can be processed through both pipelines to enable different query approaches on the same data
- Queries can be directed to either pipeline depending on the type of information needed

## Common Workflows

### Adding a New Document Source
1. Configure the source in appsettings.json
2. Create BlobStorageProcessor instances for both tabular and standard pipelines
3. Call ProcessBlobsFromStorageAsync() or ProcessFilesFromLocalDirectoryAsync() on each processor instance

### Implementing a Custom Decoder
1. Create a new class implementing IContentDecoder
2. Override SupportsMimeType() to indicate supported file types
3. Implement DecodeAsync() to process the content
4. Register the decoder in the memory pipeline

### Adding a New Query Type
1. Extend the filter generation prompt to handle the new query pattern
2. Update the KernelMemoryQueryProcessor to process the new query type
3. Test with sample queries to verify accuracy

## Troubleshooting Guide

### Common Issues

#### "Failed to load Excel file"
- Check if the file contains PivotTables (now handled with SkipPivotTables option)
- Verify the file is not corrupted or password-protected
- Try opening and resaving the file in Excel

#### "Could not access the memory DB instance"
- Verify that the memory instance is properly initialized
- Check that the correct memory implementation is being used
- Ensure the configuration is properly loaded
- For TabularExcelDecoder, ensure the IMemoryDb instance is properly resolved in dependency injection
- Remember that TabularExcelDecoder now works with the IMemoryDb interface rather than the concrete AzureCosmosDbTabularMemory class

#### "Filter generation error"
- Check the format of the query
- Verify that the filter generation prompt is properly formatted
- Look for complex or ambiguous field names in the query

#### "Type compatibility issues"
- When extending the TabularExcelDecoder, be aware of accessibility conflicts between internal and public types
- Use object parameters with casting for public methods that need to work with internal types
- Avoid exposing internal types in public method signatures

#### "Collection type pattern matching issues"
- **Problem**: C# 8's pattern matching syntax can fail with errors like `error CS8121: An expression of type 'List<string?>' cannot be handled by a pattern of type 'string'` when working with nullable generic collections
- **Solution**: Instead of using pattern matching, use a more robust type-checking approach:
  1. Get the runtime type using `GetType()` on the object
  2. Check for string type first with `object.GetType() == typeof(string)`
  3. For collections, use `typeof(System.Collections.IEnumerable).IsAssignableFrom(type) && type != typeof(string)`
  4. Extract values using `GetEnumerator()` and `MoveNext()` for collections
  5. Fall back to `ToString()` for other types
- Example:
  ```csharp
  // Instead of this problematic pattern matching:
  if (obj is string strValue) {
      // Handle string
  } else if (obj is List<string> listValue) {
      // Handle list - might fail with nullable generic types
  }
  
  // Use this robust approach:
  object objValue = obj;
  Type objType = objValue.GetType();
  
  if (objType == typeof(string)) {
      string strValue = (string)objValue;
      // Handle string
  } else if (typeof(System.Collections.IEnumerable).IsAssignableFrom(objType) && 
           objType != typeof(string)) {
      var enumerable = (System.Collections.IEnumerable)objValue;
      var enumerator = enumerable.GetEnumerator();
      // Handle collection by using the enumerator
  }
  ```
- This approach is more robust and works consistently with all collection types including nullable generic types

#### "Empty source dictionary"
- This issue has been fixed by improving the text field parsing in AzureCosmosDbTabularMemory
- The code now extracts metadata (worksheet, row, schema ID, import batch ID) from the text field first
- It also implements fallbacks to check payload fields if not found in text
- If you encounter this issue, ensure you're using the latest version of the code

#### "Empty data dictionary" or "Data parsing artifacts"
- These issues have been fixed by improving the text field parsing in `ParseSentenceFormat`
- The code now uses a more robust approach to extract data from the text field:
  1. Isolates the data section (everything after the colon)
  2. Splits the data section by periods to identify individual key-value pairs
  3. Processes each pair to extract the key and value
  4. Skips keys that look like record prefixes to avoid parsing artifacts
- If you encounter these issues, ensure you're using the latest version of the code

#### "Azure service errors"
- Verify that all API keys and endpoints are correctly configured.
- Check Azure service status for any outages.
- Ensure that rate limits have not been exceeded.

#### "Rate limiting warnings in logs"
- These warnings are normal when processing large datasets
- The system implements retry with backoff based on Retry-After headers
- Consider adding rate limiting configuration in appsettings.json
- For large datasets, consider batch processing with delays

#### Reflection Usage in TabularFilterHelper
- The `TabularFilterHelper` uses reflection to access the internal `_memoryDb` field of `IKernelMemory`. This could break if the internal structure of `KernelMemory` changes in future versions. Consider alternatives if this becomes an issue.

#### Reflection Usage in Program.cs
- The `GetMemoryDbFromKernelMemory` and `SetMemoryOnTabularExcelDecoders` methods use reflection to access internal fields and methods of the Kernel Memory framework.
- This approach is used to find and update TabularExcelDecoder instances in the pipeline at runtime.
- While this works, it's a brittle solution that depends on the internal structure of the framework.
- The methods now work with the IMemoryDb interface rather than the concrete AzureCosmosDbTabularMemory class, improving flexibility.
- Consider requesting a more robust API from the Kernel Memory team for accessing and configuring pipeline components.
